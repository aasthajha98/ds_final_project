{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj1GVnBvSgKL"
   },
   "outputs": [],
   "source": [
    "#set up packages\n",
    "import pandas as pd\n",
    "from googleapiclient import discovery\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from dotenv import load_dotenv   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert your own API Key here\n",
    "# load keys from  environmental var\n",
    "load_dotenv() # .env file in cwd\n",
    "api_key = os.environ.get(\"api_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16379,
     "status": "ok",
     "timestamp": 1701655288814,
     "user": {
      "displayName": "Aastha Jha",
      "userId": "05959336164976946971"
     },
     "user_tz": 300
    },
    "id": "pWoF9KEgg2bN",
    "outputId": "c4bf2b64-39a6-4448-afbd-ba744fb5ddf2"
   },
   "outputs": [],
   "source": [
    "#import csv with comment data\n",
    "# Read CSV into Pandas DataFrame\n",
    "df = pd.read_csv('data_w_roberta_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1701655288815,
     "user": {
      "displayName": "Aastha Jha",
      "userId": "05959336164976946971"
     },
     "user_tz": 300
    },
    "id": "IzjSkd1jtady",
    "outputId": "0f24f36a-1898-4d39-af2c-a2590b89565a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oqslm-EegHPU"
   },
   "outputs": [],
   "source": [
    "def perspective_score(text):\n",
    "    \"\"\"\n",
    "    Get Perspective API toxicity score for text.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The text you want toxicity score on.\n",
    "\n",
    "    Returns:\n",
    "    - float or None: The toxicity score if available, or None if there's an issue with the request.\n",
    "    \"\"\"\n",
    "    api_url = \"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze\"\n",
    "    params = {'key': cloud_api}\n",
    "    data = {\n",
    "        'comment': {'text': text},\n",
    "        'languages': ['en'],\n",
    "        'requestedAttributes': {'TOXICITY': {}, 'IDENTITY_ATTACK': {}}\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, params=params, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        if 'attributeScores' in results:\n",
    "            toxicity_score = results['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "            identity_attack_score = results['attributeScores']['IDENTITY_ATTACK']['summaryScore']['value']\n",
    "            score = {'TOXICITY': toxicity_score, 'IDENTITY_ATTACK': identity_attack_score}\n",
    "            return score\n",
    "\n",
    "    # Introduce a pause to avoid exceeding rate limits\n",
    "    time.sleep(1)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QKGxertrN21"
   },
   "outputs": [],
   "source": [
    "def perspective_score_batch(chunk):\n",
    "    \"\"\"\n",
    "    Get Perspective API scores for a batch of texts.\n",
    "\n",
    "      Parameters:\n",
    "      - chunk: A dataframe containing a 'text' column with text to analyze.\n",
    "\n",
    "     Returns:\n",
    "     - list: A list of dictionaries containing 'TOXICITY' and 'IDENTITY_ATTACK' scores for each text.\n",
    "    \"\"\"\n",
    "    api_url = \"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze\"\n",
    "    params = {'key': cloud_api}\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for text in tqdm(chunk['text_string']):\n",
    "        data = {\n",
    "            'comment': {'text': text},\n",
    "            'languages': ['en'],\n",
    "            'requestedAttributes': {'TOXICITY': {},'IDENTITY_ATTACK': {}}\n",
    "        }\n",
    "        response = requests.post(api_url, params=params, json=data)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()\n",
    "            if 'attributeScores' in results:\n",
    "                toxicity_score = results['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "                identity_attack_score = results['attributeScores']['IDENTITY_ATTACK']['summaryScore']['value']\n",
    "                scores.append({'TOXICITY': toxicity_score, 'IDENTITY_ATTACK': identity_attack_score})\n",
    "            else:\n",
    "                scores.append(None)\n",
    "        else:\n",
    "            scores.append(None)\n",
    "\n",
    "        # Introduce a pause to avoid exceeding rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 723224,
     "status": "ok",
     "timestamp": 1701642183483,
     "user": {
      "displayName": "Aastha Jha",
      "userId": "05959336164976946971"
     },
     "user_tz": 300
    },
    "id": "xEkR3Ii1rUy1",
    "outputId": "a076dce2-8373-49b3-dda1-f0f5aa576b01"
   },
   "outputs": [],
   "source": [
    "#split the dataframe into chunks for batch requests\n",
    "chunk_size = 5000\n",
    "chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "\n",
    "#initialize a list to store the scores\n",
    "all_scores = []\n",
    "\n",
    "#collect score for each chunk\n",
    "for chunk in chunks:\n",
    "    scores = perspective_score_batch(chunk)\n",
    "    all_scores.extend(scores)\n",
    "\n",
    "#add the scores to the dataframe\n",
    "df[['toxicity_score', 'identity_attack_score']] = pd.DataFrame(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1701645685608,
     "user": {
      "displayName": "Aastha Jha",
      "userId": "05959336164976946971"
     },
     "user_tz": 300
    },
    "id": "_p9i_a_hu7NX",
    "outputId": "6e4e1764-4f19-45db-8a13-62f167a793c7"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vr7q3Jc_oxS"
   },
   "outputs": [],
   "source": [
    "#Export to csv\n",
    "df.to_csv('data/final_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
